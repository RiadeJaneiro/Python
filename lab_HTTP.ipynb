{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20202RUHHabI",
        "outputId": "76e79798-e85a-433f-df09-f1c8b81351b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Заголовки на странице:\n",
            "Example Domain\n",
            "\n",
            "Ссылки на странице:\n",
            "More information...: https://www.iana.org/domains/example\n"
          ]
        }
      ],
      "source": [
        "# Задание 1: Базовый парсинг\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://example.com\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Извлечение заголовков\n",
        "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "print(\"Заголовки на странице:\")\n",
        "for header in headers:\n",
        "  print(header.text.strip())\n",
        "\n",
        "# Извлечение ссылок\n",
        "links = soup.find_all('a', href=True)\n",
        "print(\"\\nСсылки на странице:\")\n",
        "for link in links:\n",
        "  print(f\"{link.text.strip()}: {link['href']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rq6u-_Hq0J",
        "outputId": "be74a032-a8fd-4620-ad89-968fb0562cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Данные сохранены в news_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Задание 2: Парсинг данных с новостного сайта\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://news.ycombinator.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "news_items = soup.find_all('tr', class_='athing')\n",
        "\n",
        "with open('news_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Link', 'Comments'])\n",
        "\n",
        "    for item in news_items:\n",
        "        title = item.find('span', class_='titleline').find('a').text\n",
        "        link = item.find('span', class_='titleline').find('a')['href']\n",
        "        # Находим следующий tr для получения комментариев\n",
        "        next_tr = item.find_next_sibling('tr')\n",
        "        comments = next_tr.find('span', class_='subline').find_all('a')[-1].text\n",
        "        writer.writerow([title, link, comments])\n",
        "\n",
        "print(\"Данные сохранены в news_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsynjzRSM1mK",
        "outputId": "10c26ad0-51e8-470e-b4aa-503f92497725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Собрано 160 товаров\n",
            "{'name': 'Кеды', 'price': '62.53р.', 'brand': 'Shuzzi'}\n"
          ]
        }
      ],
      "source": [
        "# Задание 3 (продвинутое): Парсинг с пагинацией\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = \"https://gomel.megatop.by/catalog/deti?filters=%7B%22collection%22%3A%5B%22vesna_leto_2025%22%5D%7D&utm_source=yandex&utm_medium=cpc&utm_campaign=megatop_rozniza_nk_vesna_25_detskaya_obuv_belarus_shopping&yclid=15797168230458982399&page=\"\n",
        "products = []\n",
        "\n",
        "for page in range(1, 5):  # Парсим первые 4 страницы\n",
        "    url = base_url + str(page)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    items = soup.find_all('div', class_='px-1 py-1 col-md-4 col-lg-3 col-xl-3 col')\n",
        "\n",
        "    for item in items:\n",
        "        shoes = item.find_all('div', class_ = 'pa-0 content__title col col-12')\n",
        "        name = shoes[0].text.strip()\n",
        "        price = item.find('div', class_='product__price').text.strip()\n",
        "        brand = shoes[1].text.strip()\n",
        "\n",
        "        products.append({\n",
        "            'name': name,\n",
        "            'price': price,\n",
        "            'brand': brand\n",
        "        })\n",
        "\n",
        "    time.sleep(3)  # Задержка между запросами\n",
        "\n",
        "with open('products.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(products, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Собрано {len(products)} товаров\")\n",
        "print(products[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}